{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'28-j√§hriger Koch in San Francisco Mall tot aufgefunden'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datasets\n",
    "from datasets import load_from_disk\n",
    "\n",
    "ds = load_from_disk('/home/senft/ImpTransf/data/wmt17_de-en_cleaned.hf')\n",
    "ds['test'][0]['de']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_training_corpus(dataset, language = 'en', batch_size = 1000):\n",
    "    \"\"\"\n",
    "    args:\n",
    "        dataset (datasets.dataset_dict.DatasetDict); dataset dict,different features are the different languages\n",
    "        language (str); which language to extract from the dataset\n",
    "        batch_size (int); How big the chunks of the dataset should be\n",
    "    returns\n",
    "    training_corpus (generator); iterable where each element is a list of batch_size entries of the language\n",
    "    \"\"\"\n",
    "    training_corpus = (dataset['train'][i:i+batch_size][language] for i in range(0,len(dataset['train']), batch_size))\n",
    "    return training_corpus\n",
    "\n",
    "training_corpus = get_training_corpus(ds, 'en')\n",
    "\n",
    "type(next(iter(training_corpus)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "\n",
    "# # tokenizer = Tokenizer(BPE(unk_token = \"[UNK]\"))\n",
    "# # tokenizer.pre_tokenizer = Whitespace()\n",
    "# # trainer = BpeTrainer(vocab_size = 50000, show_progress = True,special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
    "\n",
    "# trained_tokenizer = tokenizer.train_from_iterator(\n",
    "#     training_corpus,\n",
    "#     trainer = trainer\n",
    "# )\n",
    "\n",
    "\n",
    "old_tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "new_tokenzier = old_tokenizer.train_new_from_iterator(get_training_corpus(ds), 50000)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./data/tokenizers/tokenizer_config.json',\n",
       " './data/tokenizers/special_tokens_map.json',\n",
       " './data/tokenizers/vocab.json',\n",
       " './data/tokenizers/merges.txt',\n",
       " './data/tokenizers/added_tokens.json',\n",
       " './data/tokenizers/tokenizer.json')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_tokenzier.save_pretrained('./data/tokenizers')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "imptransf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
